# Fair Comparison: Alternating Optimization vs. Moment Matching

## Overview

This directory contains scripts and configurations to perform a fair comparison between two optimization approaches:

1. **Alternating Optimization**: Iterates between optimizing assignment (Hungarian algorithm) and fitting chi tensor
2. **Moment Matching**: Directly fits chi tensor by matching statistical moments of shift distributions

## Key Changes for Fair Comparison

### Random Initialization

Both methods now use **random initialization with multiple trials**:

- **Alternating Optimization**: Random initial assignment of hyperfines to atoms (as before)
- **Moment Matching**: Random initial chi tensor satisfying physical constraints:
  - Traceless: χ_x + χ_y + χ_z = 0
  - Axiality: -χ_iso < χ_z < 2·χ_iso
  - Rhombicity: 0 < (χ_x - χ_y)/χ_z < 1

### Implementation

The `generate_random_chi_diagonal()` function in [`src/optimizers.py`](src/optimizers.py) generates valid random chi tensors:

```python
chi_init = generate_random_chi_diagonal(chi_iso=0.18013, seed=trial_seed)
```

## Running Experiments

### Quick Start

Run both experiments and generate comparison plots:

```bash
python scripts/compare_methods.py
```

This will:
1. Run 50 trials of alternating optimization
2. Run 50 trials of moment matching
3. Generate comparative plots in `results/comparison/`
4. Export summary statistics

### Individual Experiments

Run individual experiments:

```bash
# Alternating optimization (50 trials)
python scripts/run_experiments.py --config alternating_optimization_trials.yaml

# Moment matching (50 trials)
python scripts/run_experiments.py --config moment_matching_trials.yaml
```

## Configuration Files

### [`configs/alternating_optimization_trials.yaml`](configs/alternating_optimization_trials.yaml)
- 50 trials with random assignment initialization
- trust-constr optimizer
- Max 50 iterations per trial

### [`configs/moment_matching_trials.yaml`](configs/moment_matching_trials.yaml)
- 50 trials with random chi tensor initialization  
- trust-constr optimizer
- Max 100 iterations per trial
- Uses standardized moments (μ, σ, M3, M4, ...)

## Analysis Outputs

### Comparison Plots

Generated by `compare_methods.py`:

1. **`comparison_loss_distribution.png`**: Box plot and histogram of final loss values
2. **`comparison_chi_error_distribution.png`**: Distribution of chi tensor errors (Frobenius norm)
3. **`comparison_iterations.png`**: Convergence speed comparison
4. **`comparison_success_rate.png`**: Constraint satisfaction rates

### Summary Statistics

**`comparison_summary.csv`**: Contains:
- Best, mean, and median loss values
- Best, mean, and median chi errors  
- Average number of iterations
- Success rate (% with valid constraints)
- Convergence rate

### Statistical Tests

Mann-Whitney U tests (non-parametric) compare:
- Final loss distributions
- Chi error distributions

Significance threshold: α = 0.05

## Expected Findings

Based on initial testing, we expect to explore:

1. **Performance Gap**: How does moment matching compare when given fair random initialization?
2. **Convergence Behavior**: Does moment matching get stuck in local minima more often?
3. **Computational Cost**: Iteration count per trial for each method
4. **Robustness**: Which method is more sensitive to initialization?

## Key Implementation Details

### Random Chi Generation

The random chi tensor generation ensures:
- **Physical validity**: All constraints satisfied by construction
- **Diversity**: Uniform sampling from valid parameter space
- **Reproducibility**: Seed-based generation for each trial

### Constraint Validation

Both methods validate final chi tensors against:
```python
def validate_constraints(chi_diag, fitter):
    z_cons = fitter.orientation_via_axiality_constraint(chi_diag)
    quot_cons = fitter.orientation_rhombicity_constraint(chi_diag)
    trace_cons = fitter.trace_constraint(chi_diag)
    # All must be satisfied (≥ 0 for inequalities, ~0 for equality)
```

## Next Steps

After running experiments:

1. Review comparison plots to understand performance differences
2. Examine best trial from each method (`analyze_results.py`)
3. Create animations of convergence for both best and worst trials
4. Consider alternative moment formulations or optimization strategies

## Files Modified

- [`src/optimizers.py`](src/optimizers.py): Added `generate_random_chi_diagonal()`
- [`scripts/run_experiments.py`](scripts/run_experiments.py): Removed deterministic override, added random chi init
- [`scripts/compare_methods.py`](scripts/compare_methods.py): New comparative analysis script
